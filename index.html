<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Mehdi Azabou">
    <meta name="author" content="Mehdi Azabou">
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>BAMS</title>
    <link rel="shortcut icon" href="assets/üß†.ico" />
    <script src="https://kit.fontawesome.com/3875b07657.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
    <link rel="stylesheet" href="https://bulma.io/css/bulma-docs.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
    <link rel="stylesheet" href="style.css">
    <!-- Global site tag (gtag.js) - Google Analytics -->
   <script async src="https://www.googletagmanager.com/gtag/js?id=G-SMP0QT1S6Z"></script>
   <script>
      window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-SMP0QT1S6Z', {
            'page_path': 'https://multiscale-behavior.github.io/'
        });
    </script>
<script>
    function resizeIframe(obj) {
      obj.style.height = obj.contentWindow.document.documentElement.scrollHeight + 'px';
    }
  </script>
</head>
<body>

<section class="section">
    <div>
    <h1 class="page-title">
        Relax, it doesn't matter how you get there 
    </h1>
    <h2 class="title page-title">
        A new <span>self-supervised</span> approach for <span class="rainbow nowrap" id="rainbow" style="font-weight:600;">multi-timescale behavior</span> analysis
    </h2>
    </div>
    <div class="header authors">
        <a href="https://www.mehai.dev">Mehdi Azabou</a><sup>1</sup>
        <a href="https://dyerlab.gatech.edu/"> Michael Mendelson</a><sup>1</sup>
        <a href="https://www.gatech.edu/"> Nauman Ahad</a><sup>1</sup>
        <a href="https://www.initmaks.com">Maks Sorokin</a><sup>1</sup>
        <a href="https://www.deepmind.com/">Shantanu Thakoor</a><sup>2</sup>
        <a href="https://dyerlab.gatech.edu/">Carolina Urzay</a><sup>1</sup>
        <a href="https://dyerlab.gatech.edu/">Eva L. Dyer</a><sup>1,3</sup>
    </div>
    <div class="affiliations">
        <sup>1</sup> Georgia Institute of Technology, <sup>2</sup> DeepMind, <sup>3</sup> Emory University
    </div>
    <div class="links">
        <a href="https://arxiv.org/pdf/2303.08811">
            <i class="fa fa-file"></i> Paper
        </a>
        <a href="https://github.com/nerdslab/bams">
            <i class="fa fa-github"></i> 
            Code
        </a>
        <!-- <a href="https://github.com/nerdslab/myow">
            <i class="fa fa-book"></i> <br>
            Docs
        </a> -->
        <!--
        <a href="">
            <img src="imgs/colab_logo.png" style="width: 35px;"/> <br/>
            Colab
        </a>
        -->
        <!-- <a href="https://arxiv.org/abs/2303.08811">
            <i class="ai ai-arxiv"></i> 
            Arxiv
        </a> -->
    <!--    <a href="">-->
    <!--        <i class="ai ai-obp"></i> <br>-->
    <!--        Citation-->
    <!--    </a>-->
    
    </div>


</section>


<section class="section">
    <h3 class="section-title">
        Introduction
    </h3>
    <p>
    Studying the dynamics of behavior data, especially in complex and naturalistic behavior, provides rich information about movement and decision making that can be used to build insights into the link between the brain and behavior.
    </p>
    <p class="question">
    &#8594 Can we build models that can capture the dynamics of behavior, and help us derive insights from the data?
    </p>
    <p class="question">
    &#8594 Can we leverage self-supervised learning to process large amounts of high-dimensional noisy behavior data, without relying on annotations?
    </p>

    <p>
    We introduce a novel self-supervised learning framework called BAMS trained to predict the next action(s). BAMS is equiped with two latent spaces, the first captures the short-term dynamics, the second captures the long-term embeddings.
    </p>

    <div class="table-of-contents">
        <h2>Table of Contents</h2>
        <ul>
          <li><a href="#mice">üê≠ Mouse triplets</a>: Our model ranks üèÜ 1st overall in the MABe 2022 Multi-agent behavior benchmark.</li>
          <li><a href="#robots">ü§ñ Simulated legged robots</a>: We introduce a new dataset of a heterogeneous population of
            quadrupeds traversing different terrains.</li>
          <li><a href="#bandit">üé∞ Humans decision making</a>: Learning signatures of decision making from many individuals playing the same game.</li>
        </ul>
    </div>
</section>

<section class="section" id="mice">
    <!-- <div class="figure">
    <img src="static/bams_overview.png" style="width: 90%;"/>
    </div> -->
    <h3 class="section-title">
        <a href=""> </a>üê≠ Studying interacting mice
    </h3>
    <h3 class="subsection-title">
        Dataset
    </h3>
    <p>
    The mouse triplet dataset (<a href="https://data.caltech.edu/records/8kdn3-95j37">Link</a>) is part of the Multi-Agent Behavior Challenge (MABe 2022). It consists of a set of trajectories from three mice interacting in an open-field arena. 
    The recorded videos are processed to extract pose estimations and tracking data. We use these features as the input to our model.
     </p>   
     <p>Below, we show a sample sequence.</p>
    <div align="center">
        <iframe src="visualization_feats_5.html" height="350" width="1000" frameborder="0" scrolling="no" onload="resizeIframe(this)"></iframe>
        <p class="caption-a">Figure 1: Visualization of the pose data for a sample recording clip.</p>
    </div>
    <p>
    Note that the data is noisy, contains missing values and frequent identity swap issues. This is representative of the real-world data we are interested in studying.
</p> 


<h3 class="subsection-title">
    Results
</h3>

<p>
To evaluate the quality of the learned representation, we use a set of 13 readout tasks that are designed after a wide range behavioral analysis tasks. 
</p>

<p>
The model produces a d-dimensional represenation, to readout the desired label, we freeze the representation and train a single linear layer on top of the embedding. 
A good performance indicates that the label is linearly decodable from the learned embedding space, 
suggesting that the model is able to capture these factors that are known to be important.
</p>

<p>
We report the results for our model and other models below. Our model ranks first in the overall challenge and on all global tasks 
<a href="https://www.aicrowd.com/challenges/multi-agent-behavior-challenge-2022/problems/mabe-2022-mouse-triplets/leaderboards?challenge_leaderboard_extra_id=1049&challenge_round_id=1148&post_challenge=true">Link to Leaderboard</a>. 
</p>
<div>
    <iframe src="mabe_results.html" height="420" width="100%" frameborder="0" onload="resizeIframe(this)"></iframe>
    <p class="caption-b">Figure 2: Linear evaluation results.</p>
</div>

<h3 class="subsection-title">
    Visualizing the embedding spaces
</h3>

    <p>
    We use the learned embedding spaces to visualize the dynamics of the behavior. We show the embedding space of the first latent space (short-term dynamics) and the second latent space (long-term dynamics).
    </p>
    <div align="center">
        <iframe src="visualization_strain_5.html" height="350" width="1000" frameborder="0" scrolling="no" onload="resizeIframe(this)"></iframe>
        <p class="caption-a">Figure 3: Projection of the embeddings using PCA after normalization.</p>
    </div>

    <p>
    There are two things we can note here:
    </p>
    <p>
        <b>-</b> The long-term embedding is able to capture the different strains of mice. This is important because it shows that the behavior of the mice is not only driven by the environment, but also by their genetic background.
    </p><p>
        <b>-</b> While the long-term embedding quickly converges to a small region of the space, we can see more variability in the short-term embedding. This is expected because the short-term dynamics are used to capture momentary behavior like interactions between the mice.
    </p>

    <p>
        We visualize the embeddings again but we use the lights on/off to color the points.
    </p>
    <div align="center">
        <iframe src="visualization_lights_5.html" height="350" width="1000" frameborder="0" scrolling="no" onload="resizeIframe(this)"></iframe>
        <p class="caption-a">Figure 4: Projection of the embeddings using PCA after normalization.</p>

    </div>
    <p>
        <b>-</b> We can see that the long-term embedding is able to capture the effect of the lights on the behavior of the mice. This confirms what we know about mice being more active during the night and less active during the day.
    </p>
    <p>
        <b>-</b> Interestingly, we find that the light condition is encoded along the first PC, while the mouse strain is encoded along the second PC.
    </p>
</section>
<section class="section" id="robots">
    <h3 class="section-title">
        ü§ñ Studying simulated legged robots
    </h3>

    <p>To test our model's ability to separate behavioral factors that vary in complexity and contain distinct
        multi-timescale dynamics, we introduce a new dataset generated from a heterogeneous population of
        quadrupeds traversing different terrains.</p>

    <p> We use the NVIDIA's <a href="https://developer.nvidia.com/isaac-gym">Isaac Gym simulator</a>. We use two robots that differ by their morphology, ANYmal B and ANYmal C. To
        create heterogeneity in the population, we randomize the body mass of the robot as well as the target
        traversal velocity. We track a set of 24 proprioceptive features including linear and angular velocities
        of the robots' joints.</p>


    <div class="figure" align="center">
        <img src = "robots_overview.svg" width="800px" alt="Sketch of robots walking on different terrains."/>
        <p class="caption">Figure 5: Sketch of robots walking on different terrains.</p>
    </div>


    <p class="question">
        &#8594 Why this dataset?
    </p>
    <p>Simulation-based data collection enables access to information that is generally
        inaccessible or hard to acquire in a real-world setting. Unlike noisy measurements coming from the
        camera-based feature extractor in the case of the mouse dataset, physics engines do not suffer from
        the problem of noise. Instead, they provide accurate ground-truth information about the creature and
        the world state free of charge. Access to such information is at times critical for scrutinizing the
        capabilities of the learning algorithms.</p>

</section>

<section class="section" id="bandit">
    <h3 class="section-title">
        üé∞ Studying human decision making
    </h3>
    BAMS is also applicable to human decision making. Our <a href="https://arxiv.org/abs/2302.11023">work </a> will appear at the 11th International IEEE EMBS Conference on Neural Engineering (NER'23), Baltimore, Maryland, April 2023.
</section>



<section class="section">
    <h3 class="section-title">
        Cite our work
    </h3>
If you find this useful for your research, please consider citing our work:

<div class="citation">
<pre><code>@misc{azabou2023relax,
      doi = {10.48550/ARXIV.2303.08811},
      url = {https://arxiv.org/abs/2303.08811},
      author = {Azabou, Mehdi and Mendelson, Michael and Ahad, Nauman and Sorokin, Maks and Thakoor, Shantanu and Urzay, Carolina and Dyer, Eva L.},
      title = {Relax, it doesn't matter how you get there: A new self-supervised approach for multi-timescale behavior analysis},
      publisher = {arXiv},
      year = {2023}}</code></pre>
</div>

<div class="citation">
    <pre><code>@misc{mendelson2023,
      doi = {10.48550/ARXIV.2302.11023},
      url = {https://arxiv.org/abs/2302.11023},
      author = {Mendelson, Michael J and Azabou, Mehdi and Jacob, Suma and Grissom, Nicola and Darrow, David and Ebitz, Becket and Herman, Alexander and Dyer, Eva L.},
      title = {Learning signatures of decision making from many individuals playing the same game},
      publisher = {arXiv},
      year = {2023}}</code></pre>
    </div>
</section>




<!-- 
<footer class="footer">
    <div class="section content">

        <div style="font-size: 1em;">NerDS Lab</div>
    </div>
</footer> -->
  <script>
var rainbow = document.querySelector(".rainbow");
var rainbowGradient = window.getComputedStyle(rainbow).backgroundImage;

document.addEventListener("mousemove", function(e) {
  var rect = rainbow.getBoundingClientRect();
  var x = e.clientX;
  var y = e.clientY;
  var isUnder = ((x + 50) >= rect.left && (x - 50) <= rect.right);
  if (isUnder) {
    rainbow.style.backgroundImage = rainbowGradient;
  rainbow.style.backgroundPosition = x + "px " + y + "px";
  }
});

  </script>
</body>
</html>